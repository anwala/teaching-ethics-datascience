# Interpretability in Machine Learning

## Introduction

Interpretability is the degree to which a human can understand the reasoning behind the predictions made by a model. The increasing use of Machine Learning/AI models technology in high-stakes environment such as criminal justice and healthcare, means that model interpretability is crucial for fairness and trust, and assessing the validity and/or appropriateness of predictions.

Here we will learn about the concept of interpretability in Machine Learning and survey various technologies we've visited before to assess their degree of interpretability.

## Concepts

* [Molnar, Christoph. "Interpretability" in A guide for making black box models explainable. 2024.](https://christophm.github.io/interpretable-ml-book/interpretability.html) and [Molnar, Christoph. "Importance of Interpretability" in A guide for making black box models explainable. 2024.](https://christophm.github.io/interpretable-ml-book/interpretability-importance.html)

## Cases

* [Guo, Eileen and Karen Hao. "This is the Stanford vaccine algorithm that left out frontline doctors" MIT Technology Review](https://web.archive.org/web/20240425065403/https://www.technologyreview.com/2020/12/21/1015303/stanford-vaccine-algorithm/)

* [Compas Algorithm. Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner, "Machine
Bias," ProPublica (May 23, 2016)](https://web.archive.org/web/20240630145925/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

* [Dastin, Jeffrey. 2018. "Insight - Amazon scraps secret AI recruiting tool that showed bias against women." Reuters](https://web.archive.org/web/20181207170228/https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)

* [Heaven, Will Douglas. 2021. "Bias isn't the only problem with credit scoresâ€”and no, AI can't help." MIT Technology Review.](https://web.archive.org/web/20240330140827/https://www.technologyreview.com/2021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness-machine-learning/)

* [Martin, Kirsten. "Recommending an Insurrection: Facebook and Recommendation Algorithms" In Ethics of Data and Analytics, pp. 225-239. Auerbach Publications, 2022.](https://wm.primo.exlibrisgroup.com/permalink/01COWM_INST/g9pr7p/alma991033870654103196)

## In-class individual tasks

* Is interpretability needed in {vaccine distribution algorithms, recidivism prediction systems, recruitment software, credit-score formulas, social media recommendation systems}?
* Specify what interpretability should look like in the above systems.
* Does interpretability in the above sytems make them make them prone to manipulation? How can we avoid this?
