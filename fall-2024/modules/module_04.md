# Biased Datasets

## Introduction

The lifeblood of Machine Learning/AI systems is the ground truth data upon which they are trained. This means that creating a ground truth dataset is not trivial, and impacts the eventual model. Dataset builders not only have the important task of assessing the appropriateness of data sources/generation techniques, but also how well the data represents the real-world (which is not always possible to assess). 

Here, we will focus on the ways datasets can be biased, how this bias is different (if at all) from biases in the real world, who is responsible for the consequences of the bias, and possible remedies.

## Concepts

* [Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
Hanna Wallach, Hal Daum√© III, and Kate Crawford. "Datasheets for datasets." arXiv preprint arXiv:1803.09010 (2018)](https://arxiv.org/pdf/1803.09010.pdf)

## Cases

* [Hill, K. 2020. Wrongfully accused By an Algorithm. NY Times.](https://archive.md/xnVqb) OR [Listen at 1.5x speed, The Daily: Wrongfully Accused by an Algorithm](https://www.listennotes.com/podcasts/the-daily/wrongfully-accused-by-an-rPoU0HXfNME/)

* [Buolamwini, Joy, and Timnit Gebru How well do IBM, Microsoft, and Face++ AI
services guess the gender of a face?](http://gendershades.org)

* Optional: [Quach, K. 2020. MIT apologizes, permanently pulls offline huge dataset that
taught AI systems to use racist, misogynistic slurs.](https://www.theregister.com/2020/07/01/mit_dataset_removed/)

## In-class group task: discussion/questions

1. MEMO: Was the arrest morally wrong? Why or why not? Who (Police, Facial recognition system developers, etc.) is at fault?
2. Do dataset builders (e.g., academia, coorporations, individuals) have an obligation to vet datasets used in creating AI? Why or why not?